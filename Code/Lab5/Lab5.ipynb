{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SpaCy\n",
    "\n",
    "#### Wyświetlenie obrazów zostało zakomentowane, gdyż zatrzymuje kompilację następnych elementów kodu\n",
    "\n",
    "##### Zadania zostały wykonane na podstawie instruktażu ze strony:\n",
    "##### https://realpython.com/natural-language-processing-spacy-python/\n",
    "## Importy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pl_core_news_sm\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "import textacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Załadowanie polskiego modelu instancji"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "nlp = pl_core_news_sm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utworzenie pliku Doc ze Stringa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'jest', 'naturalne', 'przetwarzanie', 'języka', 'w', 'Spacy', '.']\n"
     ]
    }
   ],
   "source": [
    "introduction_text = ('To jest naturalne przetwarzanie języka w Spacy.')\n",
    "introduction_doc = nlp(introduction_text)\n",
    "print([token.text for token in introduction_doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utworzenie pliku Doc z pliku tekstowego"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'Rafal']\n"
     ]
    }
   ],
   "source": [
    "introduction_file_text = open(\"text.txt\").read()\n",
    "introduction_file_doc = nlp(introduction_file_text)\n",
    "print([token.text for token in introduction_file_doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detekcja zdań\n",
    "### Dzieje się to poprzez rozpoznanie początka oraz końca zdań w ciągu znaków"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Mój generator nie jest pierwszym tego typu ale ma dla mnie ważną zaletę: posługuje się zmielonymi tekstami współczesnymi, a nie np.\n",
      "Panem Tadeuszem, co próbowano wcześniej.\n",
      "Dodatkowo można wybrać sobie jeden z kilku stylów, żeby jak najlepiej wpasować się w pożądany charakter tworzonej strony: do wyboru są m.in.\n",
      "teksty polityczne, naukowe a także przyrodnicze.\n"
     ]
    }
   ],
   "source": [
    "about_text=('Mój generator nie jest pierwszym tego typu ale ma dla mnie ważną zaletę: posługuje się zmielonymi tekstami współczesnymi, a nie np. Panem Tadeuszem, co próbowano wcześniej. Dodatkowo można wybrać sobie jeden z kilku stylów, żeby jak najlepiej wpasować się w pożądany charakter tworzonej strony: do wyboru są m.in. teksty polityczne, naukowe a także przyrodnicze.')\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "print(len(sentences))\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Metoda rozpoznaje koniec zdania po kropce przez co zdania zawierające skróty słowne tworzą błędy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utworzenie własnego ogranicznika"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie rozumiem, ...\n",
      "dobra.\n",
      "Czy potrafisz np.\n",
      "liczyć?\n",
      "Liczę na twoją pomoc m.in.\n",
      "w matematycę\n",
      "Nie rozumiem, ... dobra.\n",
      "Czy potrafisz np.\n",
      "liczyć?\n",
      "Liczę na twoją pomoc m.in.\n",
      "w matematycę\n"
     ]
    }
   ],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '...' or token.text == 'np.' or token.text == 'm.in.':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "ellipsis_text = ('Nie rozumiem, ... dobra. Czy potrafisz np. liczyć? Liczę na twoją pomoc m.in. w matematycę')\n",
    "custom_nlp = pl_core_news_sm.load()\n",
    "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "ellipsis_doc = nlp(ellipsis_text)\n",
    "ellipsis_sentences = list(ellipsis_doc.sents)\n",
    "for sentence in ellipsis_sentences:\n",
    "    print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizacja\n",
    "\n",
    "#### Token to podstawowa jednostka tekstu, tokenizacja to rozbicie tekstu na pojedyncze tokeny\n",
    "\n",
    "### Wypisanie wszystkich kolenów obiektu Doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mój 0\n",
      "generator 4\n",
      "nie 14\n",
      "jest 18\n",
      "pierwszym 23\n",
      "tego 33\n",
      "typu 38\n",
      "ale 43\n",
      "ma 47\n",
      "dla 50\n",
      "mnie 54\n",
      "ważną 59\n",
      "zaletę 65\n",
      ": 71\n",
      "posługuje 73\n",
      "się 83\n",
      "zmielonymi 87\n",
      "tekstami 98\n",
      "współczesnymi 107\n",
      ", 120\n",
      "a 122\n",
      "nie 124\n",
      "np 128\n",
      ". 130\n",
      "Panem 132\n",
      "Tadeuszem 138\n",
      ", 147\n",
      "co 149\n",
      "próbowano 152\n",
      "wcześniej 162\n",
      ". 171\n",
      "Dodatkowo 173\n",
      "można 183\n",
      "wybrać 189\n",
      "sobie 196\n",
      "jeden 202\n",
      "z 208\n",
      "kilku 210\n",
      "stylów 216\n",
      ", 222\n",
      "żeby 224\n",
      "jak 229\n",
      "najlepiej 233\n",
      "wpasować 243\n",
      "się 252\n",
      "w 256\n",
      "pożądany 258\n",
      "charakter 267\n",
      "tworzonej 277\n",
      "strony 287\n",
      ": 293\n",
      "do 295\n",
      "wyboru 298\n",
      "są 305\n",
      "m.in 308\n",
      ". 312\n",
      "teksty 314\n",
      "polityczne 321\n",
      ", 331\n",
      "naukowe 333\n",
      "a 341\n",
      "także 343\n",
      "przyrodnicze 349\n",
      ". 361\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print (token, token.idx)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wszystkie atrybuty tokena"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mój 0 Mój  True False False Xxx True\n",
      "generator 4 generator  True False False xxxx False\n",
      "nie 14 nie  True False False xxx True\n",
      "jest 18 jest  True False False xxxx True\n",
      "pierwszym 23 pierwszym  True False False xxxx False\n",
      "tego 33 tego  True False False xxxx True\n",
      "typu 38 typu  True False False xxxx False\n",
      "ale 43 ale  True False False xxx True\n",
      "ma 47 ma  True False False xx True\n",
      "dla 50 dla  True False False xxx True\n",
      "mnie 54 mnie  True False False xxxx True\n",
      "ważną 59 ważną  True False False xxxx False\n",
      "zaletę 65 zaletę True False False xxxx False\n",
      ": 71 :  False True False : False\n",
      "posługuje 73 posługuje  True False False xxxx False\n",
      "się 83 się  True False False xxx True\n",
      "zmielonymi 87 zmielonymi  True False False xxxx False\n",
      "tekstami 98 tekstami  True False False xxxx False\n",
      "współczesnymi 107 współczesnymi True False False xxxx False\n",
      ", 120 ,  False True False , False\n",
      "a 122 a  True False False x True\n",
      "nie 124 nie  True False False xxx True\n",
      "np 128 np True False False xx False\n",
      ". 130 .  False True False . False\n",
      "Panem 132 Panem  True False False Xxxxx False\n",
      "Tadeuszem 138 Tadeuszem True False False Xxxxx False\n",
      ", 147 ,  False True False , False\n",
      "co 149 co  True False False xx True\n",
      "próbowano 152 próbowano  True False False xxxx False\n",
      "wcześniej 162 wcześniej True False False xxxx False\n",
      ". 171 .  False True False . False\n",
      "Dodatkowo 173 Dodatkowo  True False False Xxxxx False\n",
      "można 183 można  True False False xxxx True\n",
      "wybrać 189 wybrać  True False False xxxx False\n",
      "sobie 196 sobie  True False False xxxx True\n",
      "jeden 202 jeden  True False False xxxx True\n",
      "z 208 z  True False False x True\n",
      "kilku 210 kilku  True False False xxxx True\n",
      "stylów 216 stylów True False False xxxx False\n",
      ", 222 ,  False True False , False\n",
      "żeby 224 żeby  True False False xxxx True\n",
      "jak 229 jak  True False False xxx True\n",
      "najlepiej 233 najlepiej  True False False xxxx False\n",
      "wpasować 243 wpasować  True False False xxxx False\n",
      "się 252 się  True False False xxx True\n",
      "w 256 w  True False False x True\n",
      "pożądany 258 pożądany  True False False xxxx False\n",
      "charakter 267 charakter  True False False xxxx False\n",
      "tworzonej 277 tworzonej  True False False xxxx False\n",
      "strony 287 strony True False False xxxx False\n",
      ": 293 :  False True False : False\n",
      "do 295 do  True False False xx True\n",
      "wyboru 298 wyboru  True False False xxxx False\n",
      "są 305 są  True False False xx True\n",
      "m.in 308 m.in False False False x.xx False\n",
      ". 312 .  False True False . False\n",
      "teksty 314 teksty  True False False xxxx False\n",
      "polityczne 321 polityczne True False False xxxx False\n",
      ", 331 ,  False True False , False\n",
      "naukowe 333 naukowe  True False False xxxx False\n",
      "a 341 a  True False False x True\n",
      "także 343 także  True False False xxxx True\n",
      "przyrodnicze 349 przyrodnicze True False False xxxx False\n",
      ". 361 . False True False . False\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print (token, token.idx, token.text_with_ws,token.is_alpha, token.is_punct, token.is_space, token.shape_, token.is_stop)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utworzenie własnego tokenu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mój', 'generator', 'nie', 'jest', 'pierwszym', 'tego', 'typu', 'ale', 'ma', 'dla', 'mnie', 'ważną', 'zaletę', ':', 'posługuje', 'się', 'zmielonymi', 'tekstami', 'współczesnymi', ',', 'a', 'nie', 'np', '.', 'Panem', 'Tadeuszem', ',', 'co', 'próbowano', 'wcześniej', '.', 'Dodatkowo', 'można', 'wybrać', 'sobie', 'jeden', 'z', 'kilku', 'stylów', ',', 'żeby', 'jak', 'najlepiej', 'wpasować', 'się', 'w', 'pożądany', 'charakter', 'tworzonej', 'strony', ':', 'do', 'wyboru', 'są', 'm.in', '.', 'teksty', 'polityczne', ',', 'naukowe', 'a', 'także', 'przyrodnicze', '.']\n"
     ]
    }
   ],
   "source": [
    "custom_nlp= pl_core_news_sm.load()\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "def customize_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,suffix_search=suffix_re.search,infix_finditer=infix_re.finditer,token_match=None)\n",
    "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
    "custom_tokenizer_about_doc = custom_nlp(about_text)\n",
    "print([token.text for token in custom_tokenizer_about_doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop lista\n",
    "\n",
    "### Są to słowa odrzucane przez przeglądarkę, z powodu ich częstości, w celu zredukowania wielkości zbiorów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "były\n",
      "ku\n",
      "mi\n",
      "po\n",
      "nas\n",
      "wam\n",
      "iv\n",
      "niech\n",
      "acz\n",
      "natychmiast\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.pl.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usunięcie słów z tekstu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "pierwszym\n",
      "typu\n",
      "ważną\n",
      "zaletę\n",
      ":\n",
      "posługuje\n",
      "zmielonymi\n",
      "tekstami\n",
      "współczesnymi\n",
      ",\n",
      "np\n",
      ".\n",
      "Panem\n",
      "Tadeuszem\n",
      ",\n",
      "próbowano\n",
      "wcześniej\n",
      ".\n",
      "Dodatkowo\n",
      "wybrać\n",
      "stylów\n",
      ",\n",
      "najlepiej\n",
      "wpasować\n",
      "pożądany\n",
      "charakter\n",
      "tworzonej\n",
      "strony\n",
      ":\n",
      "wyboru\n",
      "m.in\n",
      ".\n",
      "teksty\n",
      "polityczne\n",
      ",\n",
      "naukowe\n",
      "przyrodnicze\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lista tokenów nie posiadająca stop słów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generator, pierwszym, typu, ważną, zaletę, :, posługuje, zmielonymi, tekstami, współczesnymi, ,, np, ., Panem, Tadeuszem, ,, próbowano, wcześniej, ., Dodatkowo, wybrać, stylów, ,, najlepiej, wpasować, pożądany, charakter, tworzonej, strony, :, wyboru, m.in, ., teksty, polityczne, ,, naukowe, przyrodnicze, .]\n"
     ]
    }
   ],
   "source": [
    "about_no_stopword_doc = [token for token in about_doc if not token.is_stop]\n",
    "print (about_no_stopword_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Lematyzacja\n",
    "\n",
    "### Jest to redukcja końcówek słów, aby sprowadzić słowo do formy podstawowej"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mój mój\n",
      "generator generator\n",
      "nie nie\n",
      "jest być\n",
      "pierwszym pierwszy\n",
      "tego ten\n",
      "typu typ\n",
      "ale ale\n",
      "ma mieć\n",
      "dla dla\n",
      "mnie ja\n",
      "ważną ważny\n",
      "zaletę zaleta\n",
      ": :\n",
      "posługuje posługiwać\n",
      "się się\n",
      "zmielonymi zmielonymi\n",
      "tekstami tekst\n",
      "współczesnymi współczesny\n",
      ", ,\n",
      "a a\n",
      "nie nie\n",
      "np np\n",
      ". .\n",
      "Panem pan\n",
      "Tadeuszem Tadeusz\n",
      ", ,\n",
      "co co\n",
      "próbowano próbować\n",
      "wcześniej wcześnie\n",
      ". .\n",
      "Dodatkowo dodatkowo\n",
      "można można\n",
      "wybrać wybrać\n",
      "sobie sobie\n",
      "jeden jeden\n",
      "z z\n",
      "kilku kilka\n",
      "stylów styl\n",
      ", ,\n",
      "żeby żeby\n",
      "jak jak\n",
      "najlepiej dobrze\n",
      "wpasować wpasować\n",
      "się się\n",
      "w w\n",
      "pożądany pożądany\n",
      "charakter charakter\n",
      "tworzonej tworzonej\n",
      "strony strona\n",
      ": :\n",
      "do do\n",
      "wyboru wybór\n",
      "są być\n",
      "m.in m.in\n",
      ". .\n",
      "teksty tekst\n",
      "polityczne polityczny\n",
      ", ,\n",
      "naukowe naukowy\n",
      "a a\n",
      "także także\n",
      "przyrodnicze przyrodniczy\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print(token, token.lemma_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Częstotliwość słów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "complete_text =('Starania o zrównoważony rozwój można sprowadzić do postulatu sprawiedliwości międzypokoleniowej. Skoro my możemy korzystać z przyrody taką, jaka jest dziś, to samo prawo powinno przysługiwać naszym dzieciom, wnukom, wnuczkom, ich dzieciom i tak dalej. Ergo nasz rozwój nie może odbywać się kosztem przyrody. Termin ‘zrównoważony’ trudno uznać w tej sytuacji za intuicyjnie zrozumiały, ale ktoś tak wymyślił kilkanaście lat temu i teraz trzeba się go trzymać nie chcąc popaść w lingwistyczno-definicyjne dywagacje. Które oczywiście świetnie nadają się na kolejny tekst. Ale ten jest o czymś innym.')\n",
    "complete_doc = nlp(complete_text)\n",
    "words = [token.text for token in complete_doc if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wyświetlenie najczęściej używanych słów\n",
    "\n",
    "##### Analizując najczęściej używane słowa, z poza stop listy, można wydedukować temat tekstu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('zrównoważony', 2), ('rozwój', 2), ('przyrody', 2), ('dzieciom', 2)]\n"
     ]
    }
   ],
   "source": [
    "common_words = word_freq.most_common(4)\n",
    "print (common_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wyświetlenie najrzadziej używanych słów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starania', 'sprowadzić', 'postulatu', 'sprawiedliwości', 'międzypokoleniowej', 'Skoro', 'możemy', 'korzystać', 'taką', 'jaka', 'samo', 'prawo', 'przysługiwać', 'naszym', 'wnukom', 'wnuczkom', 'dalej', 'Ergo', 'odbywać', 'kosztem', 'Termin', 'trudno', 'uznać', 'sytuacji', 'intuicyjnie', 'zrozumiały', 'wymyślił', 'kilkanaście', 'lat', 'trzymać', 'chcąc', 'popaść', 'lingwistyczno', 'definicyjne', 'dywagacje', 'oczywiście', 'świetnie', 'nadają', 'kolejny', 'tekst', 'czymś', 'innym']\n"
     ]
    }
   ],
   "source": [
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rozpoznawanie części mowy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mój ADJ ADJ adjective\n",
      "generator SUBST NOUN None\n",
      "nie QUB PART None\n",
      "jest FIN VERB None\n",
      "pierwszym ADJ ADJ adjective\n",
      "tego ADJ ADJ adjective\n",
      "typu SUBST NOUN None\n",
      "ale CONJ CCONJ conjunction\n",
      "ma FIN VERB None\n",
      "dla PREP ADP None\n",
      "mnie PPRON12 PRON None\n",
      "ważną ADJ ADJ adjective\n",
      "zaletę SUBST NOUN None\n",
      ": INTERP PUNCT None\n",
      "posługuje FIN VERB None\n",
      "się QUB PART None\n",
      "zmielonymi ADJ ADJ adjective\n",
      "tekstami SUBST NOUN None\n",
      "współczesnymi ADJ ADJ adjective\n",
      ", INTERP PUNCT None\n",
      "a CONJ CCONJ conjunction\n",
      "nie QUB PART None\n",
      "np BREV X None\n",
      ". INTERP PUNCT None\n",
      "Panem SUBST NOUN None\n",
      "Tadeuszem SUBST NOUN None\n",
      ", INTERP PUNCT None\n",
      "co SUBST NOUN None\n",
      "próbowano IMPS VERB None\n",
      "wcześniej ADV ADV adverb\n",
      ". INTERP PUNCT None\n",
      "Dodatkowo ADV ADV adverb\n",
      "można PRED VERB None\n",
      "wybrać INF VERB None\n",
      "sobie SIEBIE PRON None\n",
      "jeden ADJ ADJ adjective\n",
      "z PREP ADP None\n",
      "kilku NUM NUM numeral\n",
      "stylów SUBST NOUN None\n",
      ", INTERP PUNCT None\n",
      "żeby COMP SCONJ None\n",
      "jak ADV ADV adverb\n",
      "najlepiej ADV ADV adverb\n",
      "wpasować INF VERB None\n",
      "się QUB PART None\n",
      "w PREP ADP None\n",
      "pożądany ADJ ADJ adjective\n",
      "charakter SUBST NOUN None\n",
      "tworzonej ADJ ADJ adjective\n",
      "strony SUBST NOUN None\n",
      ": INTERP PUNCT None\n",
      "do PREP ADP None\n",
      "wyboru SUBST NOUN None\n",
      "są FIN VERB None\n",
      "m.in BREV X None\n",
      ". INTERP PUNCT None\n",
      "teksty SUBST NOUN None\n",
      "polityczne ADJ ADJ adjective\n",
      ", INTERP PUNCT None\n",
      "naukowe ADJ ADJ adjective\n",
      "a CONJ CCONJ conjunction\n",
      "także CONJ CCONJ conjunction\n",
      "przyrodnicze ADJ ADJ adjective\n",
      ". INTERP PUNCT None\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wyfiltrowanie słów należących do danych części mowy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generator, typu, zaletę, tekstami, Panem, Tadeuszem, co, stylów, charakter, strony, wyboru, teksty]\n",
      "[Mój, pierwszym, tego, ważną, zmielonymi, współczesnymi, jeden, pożądany, tworzonej, polityczne, naukowe, przyrodnicze]\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print(nouns)\n",
    "print(adjectives)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wizualizacja z displayCy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "about_interest_text = ('Różne osoby posiadają autentyczne interesy na ogólne tematy')\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "#displacy.serve(about_interest_doc, style='dep')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Operacje przygotowujące\n",
    "\n",
    "#### Odfiltrowanie ważnych tokenów\n",
    "#### Przeformatowanie tokenów do słów pisanych małą literą oraz zredukowanie do formy podstawowej"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['staranie', 'zrównoważony', 'rozwój', 'sprowadzić', 'postulat', 'sprawiedliwość', 'międzypokoleniowy', 'skoro', 'móc', 'korzystać', 'przyroda', 'taki', 'jaki', 'sam', 'prawo', 'przysługiwać', 'nasz', 'dziecko', 'wnuki', 'wnuczka', 'dziecko', 'daleko', 'ergo', 'rozwój', 'odbywać', 'koszt', 'przyroda', 'termin', 'zrównoważony', 'trudno', 'uznać', 'sytuacja', 'intuicyjnie', 'zrozumieć', 'wymyślić', 'kilkanaście', 'rok', 'trzymać', 'chcieć', 'popaść', 'lingwistyczny', 'definicyjny', 'dywagacja', 'oczywiście', 'świetnie', 'nadawać', 'kolejny', 'tekst', 'coś', 'inny']\n"
     ]
    }
   ],
   "source": [
    "def is_token_allowed(token):\n",
    "    if not token or not token.string.strip() or token.is_stop or token.is_punct:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [preprocess_token(token)for token in complete_doc if is_token_allowed(token)]\n",
    "print(complete_filtered_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ekstracja ważnych danych\n",
    "#### Ekstrakcja Imienia i nazwiska"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{'POS': 'PROPN '}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "newText=\"Rafał Kostrzyński znajduje się teraz w Berlinie\"\n",
    "newTextDoc=nlp(newText)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "print(extract_full_name(newTextDoc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Ekstrakcja numeru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "'(123) 456-789'"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberText=(\"Nie jestem pewny dlaczego nie działa lecz chciałbym ci podać swój numer telefonu (123) 456-789 \")\n",
    "\n",
    "def extract_phone_number(nlp_doc):\n",
    "    pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
    "                    {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
    "                     {'ORTH': '-', 'OP': '?'},\n",
    "                     {'SHAPE': 'ddd'}]\n",
    "    matcher.add('PHONE_NUMBER', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "conference_org_doc = nlp(numberText)\n",
    "extract_phone_number(conference_org_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ekstrakcja właściwości zależnie od gramatycznej struktury zdania\n",
    "\n",
    "#### Utworzona zostanie struktura drzewna gdzie czasownik pełni funkcję korzenia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rafal SUBST uczy nsubj\n",
      "się QUB uczy expl:pv\n",
      "uczy FIN uczy ROOT\n",
      "recytacji SUBST uczy obj\n"
     ]
    }
   ],
   "source": [
    "teaching_text = 'Rafal się uczy recytacji'\n",
    "teaching_doc = nlp(teaching_text)\n",
    "for token in teaching_doc:\n",
    "    print(token.text, token.tag_,token.head.text, token.dep_)\n",
    "#displacy.serve(teaching_doc, style='dep')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nawigacja drzew oraz poddrzew"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "one_line_about_text = ('Rafał Kostrzyński programuje w javie oraz pracuje we francuskiej firmie')\n",
    "one_line_about_doc = nlp(one_line_about_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ekstrakcja dzieci od \"programuje\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rafał', 'javie', 'pracuje']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in one_line_about_doc[2].children])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ekstrakcja poprzedniego, sąsiedniego węzłą"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kostrzyński\n"
     ]
    }
   ],
   "source": [
    "print (one_line_about_doc[2].nbor(-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ekstrakcja sąsiedniego węzłą"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n"
     ]
    }
   ],
   "source": [
    "print (one_line_about_doc[2].nbor())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ekstrakcja wszystkich tokenów na lewo od \"programuje\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rafał']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in one_line_about_doc[2].lefts])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ekstrakcja wszystkich tokenów na prawo od \"programuje\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['javie', 'pracuje']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in one_line_about_doc[2].rights])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wyświetlenie poddrzewa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rafał, Kostrzyński, programuje, w, javie, oraz, pracuje, we, francuskiej, firmie]\n"
     ]
    }
   ],
   "source": [
    "print (list(one_line_about_doc[2].subtree))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Metoda tworząca zdanie z poddrzewa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rafał Kostrzyński programuje w javie oraz pracuje we francuskiej firmie\n"
     ]
    }
   ],
   "source": [
    "def flatten_tree(tree):\n",
    "    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n",
    "print (flatten_tree(one_line_about_doc[2].subtree))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ekstrakcja zdań z nieustrukturyzowanego tekstu\n",
    "#### Detekcja zdań z fazą rzeczownika"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "conference_text =('Odbyje się konferencja deweloperska dnia 21. czerwca 2020 w Londynie')\n",
    "conference_doc = nlp(conference_text)\n",
    "for chunk in conference_doc.noun_chunks:\n",
    "    print (chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Detekcja fazy czasownika"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "przedstawi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafal Kostrzynski\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\textacy\\extract.py:338: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n"
     ]
    }
   ],
   "source": [
    "about_talk_text=('Ta rozmowa przedstawi czytelnikowi informacje na temat użycia naturalnego przetwarzania języka w Fintech')\n",
    "pattern = r'(<VERB>?<ADV>*<VERB>+)'\n",
    "about_talk_doc = textacy.make_spacy_doc(about_talk_text,lang='pl_core_news_sm')\n",
    "verb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, pattern)\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk.text)\n",
    "\n",
    "for chunk in about_talk_doc.noun_chunks:\n",
    "    print (chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detekcja Encji nazwy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akademia 0 8 orgName None\n",
      "Gdyni 33 38 placeName None\n",
      "Londynie 43 51 placeName None\n"
     ]
    }
   ],
   "source": [
    "piano_class_text=('Akademia muzyczna znajduje się w Gdyni lub Londynie i ma najlepszych instruktorów gry na pianinie')\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "for ent in piano_class_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char,ent.label_, spacy.explain(ent.label_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Informacje na temat encji"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "#displacy.serve(piano_class_doc, style='ent')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Użycie NER do ukrycia osobistych informacji"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "'Okazuje się, że Rafał Kostrzyński, Julie Fuller oraz Benjamin Brooks lubią jabłka. Kelly Cox i Matthew Evans lubią pomarańcze'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_text =('Okazuje się, że Rafał Kostrzyński, Julie Fuller oraz Benjamin Brooks lubią jabłka. Kelly Cox i Matthew Evans lubią pomarańcze')\n",
    "def replace_person_names(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'PERSON':\n",
    "        return '[REDACTED] '\n",
    "    return token.string\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    for ent in nlp_doc.ents:\n",
    "        ent.merge()\n",
    "    tokens = map(replace_person_names, nlp_doc)\n",
    "    return ''.join(tokens)\n",
    "\n",
    "survey_doc = nlp(survey_text)\n",
    "redact_names(survey_doc)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c06808bb",
   "language": "python",
   "display_name": "PyCharm (ML_Lecture)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}